{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HashTag Creator - Basic Solution\n",
    "By Ivelize\n",
    "\n",
    "This solution is focused on show how an inverted index problem could be solve without API.\n",
    "\n",
    "#### Problem definition: given a set of txt files, find the \"most common\" occurring words, and the sentences where they are used.\n",
    "\n",
    "Note: The algorithm's criteria to sort the words is: (i) the words which occur in the highest quantity in terms of files number, and (ii) the words which occur in the highest quantity in terms of absoluty number.\n",
    "\n",
    "#### Assumption: \n",
    " - the files must be in the same notebook's directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from collections import OrderedDict\n",
    "import pandas as pd\n",
    "import re\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stopwords = {\"a\", \"about\", \"above\", \"above\", \"across\", \"after\", \"afterwards\", \"again\", \"against\", \"all\", \"almost\", \"alone\", \"along\", \"already\", \"also\",\"although\",\"always\",\"am\",\"among\", \"amongst\", \"amoungst\", \"amount\",  \"an\", \"and\", \"another\", \"any\",\"anyhow\",\"anyone\",\"anything\",\"anyway\", \"anywhere\", \"are\", \"around\", \"as\",  \"at\", \"back\",\"be\",\"became\", \"because\",\"become\",\"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"behind\", \"being\", \"below\", \"beside\", \"besides\", \"between\", \"beyond\", \"bill\", \"both\", \"bottom\",\"but\", \"by\", \"call\", \"can\", \"cannot\", \"cant\", \"co\", \"con\", \"could\", \"couldnt\", \"cry\", \"de\", \"describe\", \"detail\", \"do\", \"don't\", \"done\", \"down\", \"due\", \"during\", \"each\", \"eg\", \"eight\", \"either\", \"eleven\",\"else\", \"elsewhere\", \"empty\", \"enough\", \"etc\", \"even\", \"ever\", \"every\", \"everyone\", \"everything\", \"everywhere\", \"except\", \"few\", \"fifteen\", \"fify\", \"fill\", \"find\", \"fire\", \"first\", \"five\", \"for\", \"former\", \"formerly\", \"forty\", \"found\", \"four\", \"from\", \"front\", \"full\", \"further\", \"get\", \"give\", \"go\", \"had\", \"has\", \"hasnt\", \"have\", \"he\", \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"however\", \"hundred\", \"i\", \"ie\", \"if\", \"in\", \"inc\", \"indeed\", \"interest\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \"just\", \"keep\", \"last\", \"latter\", \"latterly\", \"let\", \"least\", \"less\", \"ltd\", \"made\", \"many\", \"may\", \"me\", \"meanwhile\", \"might\", \"mill\", \"mine\", \"more\", \"moreover\", \"most\", \"mostly\", \"move\", \"much\", \"must\", \"my\", \"myself\", \"name\", \"namely\", \"neither\", \"never\", \"nevertheless\", \"next\", \"nine\", \"no\", \"nobody\", \"none\", \"noone\", \"nor\", \"not\", \"nothing\", \"now\", \"nowhere\", \"of\", \"off\", \"often\", \"on\", \"once\", \"one\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\",\"part\", \"per\", \"perhaps\", \"please\", \"put\", \"rather\", \"re\", \"same\", \"see\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"serious\", \"several\", \"she\", \"should\", \"show\", \"side\", \"since\", \"sincere\", \"six\", \"sixty\", \"so\", \"some\", \"somehow\", \"someone\", \"something\", \"sometime\", \"sometimes\", \"somewhere\", \"still\", \"such\", \"system\", \"take\", \"ten\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\", \"therefore\", \"therein\", \"thereupon\", \"these\", \"they\", \"thickv\", \"thin\", \"third\", \"this\", \"those\", \"though\", \"three\", \"through\", \"throughout\", \"thru\", \"thus\", \"to\", \"together\", \"too\", \"top\", \"toward\", \"towards\", \"twelve\", \"twenty\", \"two\", \"un\", \"under\", \"until\", \"up\", \"upon\", \"us\", \"very\", \"via\", \"was\", \"we\", \"we're\", \"we've\", \"well\", \"were\", \"what\", \"whatever\", \"when\", \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whither\", \"who\", \"whoever\", \"whole\", \"whom\", \"whose\", \"why\", \"will\", \"with\", \"within\", \"without\", \"would\", \"yet\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"-\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Reading the txt files\n",
    "list_setences_per_file = list()\n",
    "dict_words_frequency_per_file = {}\n",
    "for filename in glob.glob('*.txt'):\n",
    "    with open(filename) as f:\n",
    "        words_frequency = Counter(f.read().split())\n",
    "        f.seek(0)\n",
    "        sentences = re.split('(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?|\\!)\\s', f.read())\n",
    "    # remove whitespace characters like `\\n` at the end of each line\n",
    "    sentences = [x.strip() for x in sentences]\n",
    "    list_setences_per_file.extend(sentences)\n",
    "    dict_words_frequency_per_file[filename.split('.')[0]] = words_frequency\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dict_words_cleaned = {}\n",
    "for fileid, words_freq in dict_words_frequency_per_file.iteritems(): \n",
    "    for k,v in words_freq.iteritems():\n",
    "        word = k.lower()      \n",
    "        if word not in stopwords:\n",
    "            info={}\n",
    "            if word in dict_words_cleaned.keys():\n",
    "                value = dict_words_cleaned.pop(word)\n",
    "                info['files'] = value['files']\n",
    "                if fileid not in value['files']:\n",
    "                    info['files'].append(fileid)\n",
    "                info['sentence'] = []\n",
    "                info['file_freq'] = value['file_freq'] + 1\n",
    "                info['word_freq'] = value['word_freq'] + v\n",
    "                dict_words_cleaned[word] = info\n",
    "            else:\n",
    "                info['files'] = [fileid]\n",
    "                info['sentence'] = []\n",
    "                info['file_freq'] = 1\n",
    "                info['word_freq'] = v \n",
    "                dict_words_cleaned[word]= info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for k,v in dict_words_cleaned.items():\n",
    "    for sentence in list_setences_per_file:\n",
    "        if k in sentence:\n",
    "            dict_words_cleaned[k]['sentence'].append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dict_words_cleaned = OrderedDict(sorted(dict_words_cleaned.items(), key=lambda x: (x[1]['file_freq'], x[1]['word_freq']), reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dict_words_cleaned, index=['files', 'sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>like</th>\n",
       "      <th>right</th>\n",
       "      <th>tell</th>\n",
       "      <th>good</th>\n",
       "      <th>now,</th>\n",
       "      <th>thank</th>\n",
       "      <th>people</th>\n",
       "      <th>american</th>\n",
       "      <th>government</th>\n",
       "      <th>...</th>\n",
       "      <th>inspiration</th>\n",
       "      <th>bulwark</th>\n",
       "      <th>resolution</th>\n",
       "      <th>baker</th>\n",
       "      <th>slumber,</th>\n",
       "      <th>craft</th>\n",
       "      <th>diploma.</th>\n",
       "      <th>news.</th>\n",
       "      <th>dismissal</th>\n",
       "      <th>\"hey,</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>files</th>\n",
       "      <td>[doc2, doc3, doc1, doc6, doc4, doc5]</td>\n",
       "      <td>[doc2, doc3, doc1, doc6, doc4, doc5]</td>\n",
       "      <td>[doc2, doc3, doc1, doc6, doc4, doc5]</td>\n",
       "      <td>[doc2, doc3, doc1, doc6, doc4, doc5]</td>\n",
       "      <td>[doc2, doc3, doc1, doc6, doc4, doc5]</td>\n",
       "      <td>[doc2, doc3, doc1, doc6, doc5]</td>\n",
       "      <td>[doc2, doc3, doc6, doc4, doc5]</td>\n",
       "      <td>[doc2, doc3, doc1, doc6, doc4, doc5]</td>\n",
       "      <td>[doc2, doc3, doc1, doc6, doc4, doc5]</td>\n",
       "      <td>[doc2, doc3, doc1, doc6, doc4, doc5]</td>\n",
       "      <td>...</td>\n",
       "      <td>[doc2]</td>\n",
       "      <td>[doc5]</td>\n",
       "      <td>[doc5]</td>\n",
       "      <td>[doc5]</td>\n",
       "      <td>[doc1]</td>\n",
       "      <td>[doc5]</td>\n",
       "      <td>[doc2]</td>\n",
       "      <td>[doc2]</td>\n",
       "      <td>[doc4]</td>\n",
       "      <td>[doc6]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentence</th>\n",
       "      <td>[I became a civil rights lawyer, and taught co...</td>\n",
       "      <td>[We welcomed immigrants to our shores, we open...</td>\n",
       "      <td>[I became a civil rights lawyer, and taught co...</td>\n",
       "      <td>[But let me tell you how I came to be here., I...</td>\n",
       "      <td>[And if you will join me in this improbable qu...</td>\n",
       "      <td>[As most of you know, I am not a native of thi...</td>\n",
       "      <td>[Let me begin by saying thanks to all you who'...</td>\n",
       "      <td>[In the face of a politics that's shut you out...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[I saw that the problems people faced weren't ...</td>\n",
       "      <td>...</td>\n",
       "      <td>[Let me express my thanks to the historic slat...</td>\n",
       "      <td>[The notion that Iraq would quickly and easily...</td>\n",
       "      <td>[At that point, seventy-five U.S. Senators, Re...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[And if you will join me in this improbable qu...</td>\n",
       "      <td>[I am committed to working with this White Hou...</td>\n",
       "      <td>[We measure progress by how many people can fi...</td>\n",
       "      <td>[This country is more generous than one where ...</td>\n",
       "      <td>[This coalition succeeded by promising change,...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 4071 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       time  \\\n",
       "files                  [doc2, doc3, doc1, doc6, doc4, doc5]   \n",
       "sentence  [I became a civil rights lawyer, and taught co...   \n",
       "\n",
       "                                                       like  \\\n",
       "files                  [doc2, doc3, doc1, doc6, doc4, doc5]   \n",
       "sentence  [We welcomed immigrants to our shores, we open...   \n",
       "\n",
       "                                                      right  \\\n",
       "files                  [doc2, doc3, doc1, doc6, doc4, doc5]   \n",
       "sentence  [I became a civil rights lawyer, and taught co...   \n",
       "\n",
       "                                                       tell  \\\n",
       "files                  [doc2, doc3, doc1, doc6, doc4, doc5]   \n",
       "sentence  [But let me tell you how I came to be here., I...   \n",
       "\n",
       "                                                       good  \\\n",
       "files                  [doc2, doc3, doc1, doc6, doc4, doc5]   \n",
       "sentence  [And if you will join me in this improbable qu...   \n",
       "\n",
       "                                                       now,  \\\n",
       "files                        [doc2, doc3, doc1, doc6, doc5]   \n",
       "sentence  [As most of you know, I am not a native of thi...   \n",
       "\n",
       "                                                      thank  \\\n",
       "files                        [doc2, doc3, doc6, doc4, doc5]   \n",
       "sentence  [Let me begin by saying thanks to all you who'...   \n",
       "\n",
       "                                                     people  \\\n",
       "files                  [doc2, doc3, doc1, doc6, doc4, doc5]   \n",
       "sentence  [In the face of a politics that's shut you out...   \n",
       "\n",
       "                                      american  \\\n",
       "files     [doc2, doc3, doc1, doc6, doc4, doc5]   \n",
       "sentence                                    []   \n",
       "\n",
       "                                                 government   ...    \\\n",
       "files                  [doc2, doc3, doc1, doc6, doc4, doc5]   ...     \n",
       "sentence  [I saw that the problems people faced weren't ...   ...     \n",
       "\n",
       "                                                inspiration  \\\n",
       "files                                                [doc2]   \n",
       "sentence  [Let me express my thanks to the historic slat...   \n",
       "\n",
       "                                                    bulwark  \\\n",
       "files                                                [doc5]   \n",
       "sentence  [The notion that Iraq would quickly and easily...   \n",
       "\n",
       "                                                 resolution   baker  \\\n",
       "files                                                [doc5]  [doc5]   \n",
       "sentence  [At that point, seventy-five U.S. Senators, Re...      []   \n",
       "\n",
       "                                                   slumber,  \\\n",
       "files                                                [doc1]   \n",
       "sentence  [And if you will join me in this improbable qu...   \n",
       "\n",
       "                                                      craft  \\\n",
       "files                                                [doc5]   \n",
       "sentence  [I am committed to working with this White Hou...   \n",
       "\n",
       "                                                   diploma.  \\\n",
       "files                                                [doc2]   \n",
       "sentence  [We measure progress by how many people can fi...   \n",
       "\n",
       "                                                      news.  \\\n",
       "files                                                [doc2]   \n",
       "sentence  [This country is more generous than one where ...   \n",
       "\n",
       "                                                  dismissal   \"hey,  \n",
       "files                                                [doc4]  [doc6]  \n",
       "sentence  [This coalition succeeded by promising change,...      []  \n",
       "\n",
       "[2 rows x 4071 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

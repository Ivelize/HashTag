{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## HashTag Creator\n",
    "By Ivelize\n",
    "\n",
    "#### Problem definition: given a set of txt files, find \"most relevant\" words, and the sentences where they are used.\n",
    "\n",
    "The algorithm's criteria to sort words is according to TF-IDF, which was implemented with scikit-learn framework.\n",
    "\n",
    "#### Assumption: \n",
    "   - the files should be in the same notebook's directory and encoded in 'ascii';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "import collections\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Reading the txt files\n",
    "files_content = list()\n",
    "dict_setences_per_file = {}\n",
    "for filename in glob.glob('*.txt'):\n",
    "    with open(filename) as f:\n",
    "        doc_text = f.readlines()\n",
    "        f.seek(0)\n",
    "        sentences = re.split('(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?|\\!)\\s', f.read())\n",
    "    f.close()\n",
    "    # remove whitespace characters like `\\n` at the end of each line\n",
    "    doc_text = [x.strip() for x in doc_text]\n",
    "    sentences = [x.strip() for x in sentences]\n",
    "    files_content.extend(doc_text)\n",
    "    dict_setences_per_file[filename.split('.')[0]] = sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tokenize and counting the word occurrences with tf-idf normalization\n",
    "tf = TfidfVectorizer(analyzer='word', stop_words='english', min_df=0.1, lowercase=True)\n",
    "words_tf = tf.fit_transform(files_content)\n",
    "features_per_doc = tf.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Selecting the words that are most important in the rows of tfidf matrix\n",
    "tfidf_means_docs = []\n",
    "for w_tf in words_tf:\n",
    "    tfidf_means = np.mean(w_tf.toarray(), axis=0)\n",
    "    tfidf_means_docs.append(tfidf_means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sorting 100 most representative words of each document\n",
    "topn_ids = []\n",
    "feature_qty = 100\n",
    "for tfidf in tfidf_means_docs:\n",
    "    topn_ids.append(np.argsort(tfidf)[::-1][:feature_qty])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Selecting the top features\n",
    "list_top_feats = []\n",
    "for t_id in topn_ids:\n",
    "    features = features_per_doc\n",
    "    top_feats = [(features[i]) for i in t_id]\n",
    "    list_top_feats.extend(top_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u've': 310, u'just': 310, u'people': 310, u'country': 310, u'work': 310, u'know': 310, u'time': 310, u'america': 310}\n"
     ]
    }
   ],
   "source": [
    "# Counting the occurrency of the most relevant features of the files\n",
    "top_feats_counted = collections.Counter(list_top_feats)\n",
    "threshold_occurrence_qty = 1\n",
    "dict_feats = dict((k, v) for k, v in top_feats_counted.items() if v > threshold_occurrence_qty)\n",
    "print(dict_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dict_final = {}\n",
    "for doc, list_sentences in dict_setences_per_file.items():\n",
    "    for sentence in list_sentences:\n",
    "        for word, freq in dict_feats.items():\n",
    "            if word in sentence:\n",
    "                info={}\n",
    "                if word in dict_final.keys():\n",
    "                    value = dict_final.pop(word)\n",
    "                    info['files'] = value['files']\n",
    "                    if doc not in value['files']:\n",
    "                        info['files'].append(doc)\n",
    "                    info['sentence'] = value['sentence']\n",
    "                    if doc not in value['sentence']:\n",
    "                        info['sentence'].append(sentence)\n",
    "                    dict_final[word] = info\n",
    "                else:\n",
    "                    info['files'] = [doc]\n",
    "                    info['sentence'] = [sentence]\n",
    "                    dict_final[word]= info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dict_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>just</th>\n",
       "      <th>know</th>\n",
       "      <th>people</th>\n",
       "      <th>time</th>\n",
       "      <th>ve</th>\n",
       "      <th>work</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>files</th>\n",
       "      <td>[doc2, doc3, doc1, doc4]</td>\n",
       "      <td>[doc2, doc3, doc1, doc6, doc4]</td>\n",
       "      <td>[doc2, doc3, doc1, doc6, doc4]</td>\n",
       "      <td>[doc2, doc3, doc1, doc6, doc4]</td>\n",
       "      <td>[doc2, doc3, doc1, doc6, doc4]</td>\n",
       "      <td>[doc2, doc3, doc1, doc6, doc4]</td>\n",
       "      <td>[doc2, doc3, doc1, doc6, doc4]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentence</th>\n",
       "      <td>[It is that promise that has always set this c...</td>\n",
       "      <td>[And when one of his chief advisors - the man ...</td>\n",
       "      <td>[Four years ago, I stood before you and told y...</td>\n",
       "      <td>[Tonight, I say to the American people, to Dem...</td>\n",
       "      <td>[I am grateful to finish this journey with one...</td>\n",
       "      <td>[Let me express my thanks to the historic slat...</td>\n",
       "      <td>[Let me express my thanks to the historic slat...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    country  \\\n",
       "files                              [doc2, doc3, doc1, doc4]   \n",
       "sentence  [It is that promise that has always set this c...   \n",
       "\n",
       "                                                       just  \\\n",
       "files                        [doc2, doc3, doc1, doc6, doc4]   \n",
       "sentence  [And when one of his chief advisors - the man ...   \n",
       "\n",
       "                                                       know  \\\n",
       "files                        [doc2, doc3, doc1, doc6, doc4]   \n",
       "sentence  [Four years ago, I stood before you and told y...   \n",
       "\n",
       "                                                     people  \\\n",
       "files                        [doc2, doc3, doc1, doc6, doc4]   \n",
       "sentence  [Tonight, I say to the American people, to Dem...   \n",
       "\n",
       "                                                       time  \\\n",
       "files                        [doc2, doc3, doc1, doc6, doc4]   \n",
       "sentence  [I am grateful to finish this journey with one...   \n",
       "\n",
       "                                                         ve  \\\n",
       "files                        [doc2, doc3, doc1, doc6, doc4]   \n",
       "sentence  [Let me express my thanks to the historic slat...   \n",
       "\n",
       "                                                       work  \n",
       "files                        [doc2, doc3, doc1, doc6, doc4]  \n",
       "sentence  [Let me express my thanks to the historic slat...  "
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
